name: LLM Pipeline
description: 'Build prompt, call LLM (OAuth or API fallback), parse response'

inputs:
  prompt_file:
    description: 'Path to prompt template file'
    required: true
  data_file:
    description: 'Path to data file to append to prompt'
    required: true
  output_file:
    description: 'Where to write the parsed LLM response'
    default: '/tmp/llm_response.txt'
  claude_code_oauth_token:
    description: 'Claude Code OAuth token (enables OAuth path when set)'
    default: ''
  provider:
    description: 'LLM provider for API fallback (claude, openai, gemini, vertex)'
    default: 'claude'
  model:
    description: 'Model name'
    default: 'claude-sonnet-4-5-20250929'
  max_tokens:
    description: 'Max tokens for LLM response'
    default: '4096'
  anthropic_api_key:
    description: 'Anthropic API key'
    default: ''
  openai_api_key:
    description: 'OpenAI API key'
    default: ''
  google_api_key:
    description: 'Google AI API key'
    default: ''
  google_application_credentials_json:
    description: 'Google service account credentials (base64-encoded)'
    default: ''
  vertex_project:
    description: 'Google Cloud project for Vertex AI'
    default: ''
  vertex_region:
    description: 'Google Cloud region for Vertex AI'
    default: ''

outputs:
  response_file:
    description: 'Path to the parsed response file'
    value: ${{ inputs.output_file }}

runs:
  using: composite
  steps:
    # 1. Build prompt (inside workspace so claude-code-action can access it)
    - name: Build prompt
      shell: bash
      env:
        PROMPT_FILE: ${{ inputs.prompt_file }}
        DATA_FILE: ${{ inputs.data_file }}
        PROMPT_FILE_TMP_WITH_DATA: ${{ github.workspace }}/.llm_user_prompt.txt
      run: ${{ github.action_path }}/scripts/llm-prompt-build.sh

    # 2. Auth check
    - name: Check auth method
      id: auth
      shell: bash
      run: |
        if [[ "${{ inputs.provider }}" == "claude" && -n "${{ inputs.claude_code_oauth_token }}" ]]; then
          echo "USE_OAUTH=true" >> $GITHUB_OUTPUT
        else
          echo "USE_OAUTH=false" >> $GITHUB_OUTPUT
        fi

    # 3. Call LLM (OAuth)
    - name: Call LLM (OAuth)
      id: llm_oauth
      if: steps.auth.outputs.USE_OAUTH == 'true'
      uses: anthropics/claude-code-action@v1
      with:
        claude_code_oauth_token: ${{ inputs.claude_code_oauth_token }}
        prompt: |
          Read the file .llm_user_prompt.txt and follow the instructions in it.
          Respond with ONLY the requested output (no code fences, no explanations).
        claude_args: |
          --model ${{ inputs.model }}
          --max-turns 15

    # 4. Call LLM (API fallback)
    - name: Call LLM (API)
      if: steps.auth.outputs.USE_OAUTH != 'true'
      shell: bash
      env:
        PROMPT_FILE_TMP_WITH_DATA: ${{ github.workspace }}/.llm_user_prompt.txt
        PROVIDER: ${{ inputs.provider }}
        MODEL: ${{ inputs.model }}
        MAX_TOKENS: ${{ inputs.max_tokens }}
        ANTHROPIC_API_KEY: ${{ inputs.anthropic_api_key }}
        OPENAI_API_KEY: ${{ inputs.openai_api_key }}
        GOOGLE_API_KEY: ${{ inputs.google_api_key }}
        GOOGLE_APPLICATION_CREDENTIALS_JSON: ${{ inputs.google_application_credentials_json }}
        VERTEX_PROJECT: ${{ inputs.vertex_project }}
        VERTEX_REGION: ${{ inputs.vertex_region }}
      run: ${{ github.action_path }}/scripts/llm-call.sh > /tmp/llm_raw.txt

    # 5. Parse response
    - name: Parse response
      shell: bash
      env:
        LLM_API_RESPONSE_FILE: /tmp/llm_raw.txt
        LLM_RESPONSE_CLAUDE_EXEC_FILE: ${{ steps.llm_oauth.outputs.execution_file }}
        LLM_RESPONSE_PARSED: ${{ inputs.output_file }}
      run: ${{ github.action_path }}/scripts/llm-response-parse.sh
